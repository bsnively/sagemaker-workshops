{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1570414361246_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-157-157.ec2.internal:20888/proxy/application_1570414361246_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-133-202.ec2.internal:8042/node/containerlogs/container_1570414361246_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/2019 Taxi Data Count:  452091107\n",
      "root\n",
      "|-- pickup_datetime: timestamp\n",
      "|-- dropoff_datetime: timestamp\n",
      "|-- pulocationid: long\n",
      "|-- dolocationid: long\n",
      "|-- type: string\n",
      "|-- vendorid: string"
     ]
    }
   ],
   "source": [
    "taxi_data = glueContext.create_dynamic_frame.from_catalog(database=\"reinvent19\", table_name=\"canonical\")\n",
    "print(\"2018/2019 Taxi Data Count: \", taxi_data.count())\n",
    "taxi_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The records in your input files should contain the following fields:\n",
    "\n",
    "    start—A string with the format YYYY-MM-DD HH:MM:SS. The start timestamp can't contain time zone information.\n",
    "\n",
    "    target—An array of floating-point values or integers that represent the time series. You can encode missing values as null literals, or as \"NaN\" strings in JSON, or as nan floating-point values in Parquet.\n",
    "\n",
    "    dynamic_feat (optional)—An array of arrays of floating-point values or integers that represents the vector of custom feature time series (dynamic features). If you set this field, all records must have the same number of inner arrays (the same number of feature time series). In addition, each inner array must have the same length as the associated target value. Missing values are not supported in the features. For example, if target time series represents the demand of different products, an associated dynamic_feat might be a boolean time-series which indicates whether a promotion was applied (1) to the particular product or not (0):\n",
    "\n",
    "    {\"start\": ..., \"target\": [1, 5, 10, 2], \"dynamic_feat\": [[0, 1, 1, 0]]}\n",
    "\n",
    "    cat (optional)—An array of categorical features that can be used to encode the groups that the record belongs to. Categorical features must be encoded as a 0-based sequence of positive integers. For example, the categorical domain {R, G, B} can be encoded as {0, 1, 2}. All values from each categorical domain must be represented in the training dataset. That's because the DeepAR algorithm can forecast only for categories that have been observed during training. And, each categorical feature is embedded in a low-dimensional space whose dimensionality is controlled by the embedding_dimension hyperparameter. For more information, see DeepAR Hyperparameters.\n",
    "\n",
    "If you use a JSON file, it must be in JSON Lines format. For example:\n",
    "\n",
    "{\"start\": \"2009-11-01 00:00:00\", \"target\": [4.3, \"NaN\", 5.1, ...], \"cat\": [0, 1], \"dynamic_feat\": [[1.1, 1.2, 0.5, ...]]}\n",
    "{\"start\": \"2012-01-30 00:00:00\", \"target\": [1.0, -5.0, ...], \"cat\": [2, 3], \"dynamic_feat\": [[1.1, 2.05, ...]]}\n",
    "{\"start\": \"1999-01-30 00:00:00\", \"target\": [2.0, 1.0], \"cat\": [1, 4], \"dynamic_feat\": [[1.3, 0.4]]}\n",
    "\n",
    "In this example, each time series has two associated categorical features and one time series features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+------------+------------+----+--------+\n",
      "|pickup_datetime    |dropoff_datetime|pulocationid|dolocationid|type|vendorid|\n",
      "+-------------------+----------------+------------+------------+----+--------+\n",
      "|2018-01-30 21:15:34|null            |129         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:35:29|null            |112         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:16:34|null            |42          |null        |fhv |fhv     |\n",
      "|2018-01-30 21:40:35|null            |131         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:49:59|null            |121         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:44:55|null            |235         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:51:30|null            |235         |null        |fhv |fhv     |\n",
      "|2018-01-30 22:15:28|null            |208         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:46:49|null            |265         |null        |fhv |fhv     |\n",
      "|2018-01-30 21:35:23|null            |29          |null        |fhv |fhv     |\n",
      "|2018-01-30 21:49:52|null            |21          |null        |fhv |fhv     |\n",
      "|2018-01-30 22:02:50|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:44:27|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:57:04|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 22:03:41|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 22:29:58|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:40:41|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:54:57|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 22:02:23|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:28:14|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:19:21|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 22:07:34|null            |null        |null        |fhv |fhv     |\n",
      "|2018-01-30 21:55:52|null            |64          |null        |fhv |fhv     |\n",
      "|2018-01-30 21:10:04|null            |79          |null        |fhv |fhv     |\n",
      "|2018-01-30 21:19:03|null            |4           |null        |fhv |fhv     |\n",
      "|2018-01-30 21:34:12|null            |87          |null        |fhv |fhv     |\n",
      "|2018-01-30 22:01:18|null            |37          |null        |fhv |fhv     |\n",
      "|2018-01-30 21:20:38|null            |33          |null        |fhv |fhv     |\n",
      "|2018-01-30 22:08:48|null            |78          |null        |fhv |fhv     |\n",
      "|2018-01-30 22:47:18|null            |265         |null        |fhv |fhv     |\n",
      "+-------------------+----------------+------------+------------+----+--------+\n",
      "only showing top 30 rows"
     ]
    }
   ],
   "source": [
    "taxi_data.toDF().show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to restructure this so that each time is a single row, and the time series values are in the series, followed by the numerical and categorical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first let's re-set the interval to a finer grain,  right now looking at things per second is too fine grain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = taxi_data.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max as max_, min as min_\n",
    "\n",
    "day = 60 * 60 * 24\n",
    "epoch = (col(\"pickup_datetime\").cast(\"bigint\") / day).cast(\"bigint\") * day\n",
    "\n",
    "with_epoch = df.withColumn(\"epoch\", epoch)\n",
    "\n",
    "min_epoch, max_epoch = with_epoch.select(min_(\"epoch\"), max_(\"epoch\")).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+------------+------------+------+--------+-------------------+\n",
      "|epoch    |pickup_datetime    |dropoff_datetime   |pulocationid|dolocationid|type  |vendorid|ts_resampled       |\n",
      "+---------+-------------------+-------------------+------------+------------+------+--------+-------------------+\n",
      "|978307200|2001-01-01 00:09:39|2001-01-01 06:39:54|48          |148         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:02:08|2001-01-01 01:00:02|151         |151         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:02:26|2001-01-01 00:04:49|48          |48          |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 22:55:43|2001-01-02 00:05:43|234         |162         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:01:48|2001-01-01 00:15:47|43          |170         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:05:12|2001-01-01 00:09:59|140         |229         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:13:42|2001-01-01 00:22:17|170         |161         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:06:42|2001-01-01 16:53:11|141         |236         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978307200|2001-01-01 00:07:04|2001-01-01 00:07:30|48          |163         |yellow|2       |2001-01-01 00:00:00|\n",
      "|978393600|null               |null               |null        |null        |null  |null    |2001-01-02 00:00:00|\n",
      "+---------+-------------------+-------------------+------------+------------+------+--------+-------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Reference range \n",
    "ref = spark.range(\n",
    "    min_epoch, max_epoch + 1, day\n",
    ").toDF(\"epoch\")\n",
    "\n",
    "resampled_df = (ref\n",
    "    .join(with_epoch, \"epoch\", \"left\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .withColumn(\"ts_resampled\", col(\"epoch\").cast(\"timestamp\")))\n",
    "\n",
    "resampled_df.cache()\n",
    "\n",
    "resampled_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now that we are resampling per day the resample column, in which we can now aggregate across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------+------------+-----+\n",
      "|ts_resampled       |type  |pulocationid|dolocationid|count|\n",
      "+-------------------+------+------------+------------+-----+\n",
      "|2018-01-07 00:00:00|yellow|162         |142         |127  |\n",
      "|2018-01-07 00:00:00|yellow|144         |249         |117  |\n",
      "|2018-01-07 00:00:00|yellow|138         |7           |107  |\n",
      "|2018-01-07 00:00:00|yellow|170         |90          |126  |\n",
      "|2018-01-07 00:00:00|yellow|90          |231         |89   |\n",
      "|2018-01-07 00:00:00|yellow|161         |75          |49   |\n",
      "|2018-01-07 00:00:00|yellow|114         |113         |143  |\n",
      "|2018-01-07 00:00:00|yellow|114         |209         |19   |\n",
      "|2018-01-07 00:00:00|yellow|151         |74          |59   |\n",
      "|2018-01-07 00:00:00|yellow|186         |68          |201  |\n",
      "+-------------------+------+------------+------------+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "count_per_day_resamples = resampled_df.groupBy([\"ts_resampled\", \"type\", \"pulocationid\", \"dolocationid\"]).count()\n",
    "count_per_day_resamples.cache()\n",
    "count_per_day_resamples.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO -- Right now the \"null\" column is showing up instead of the fhvhv.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------+----+-----+------+\n",
      "|ts_resampled       |pulocationid|dolocationid|fhv |green|yellow|\n",
      "+-------------------+------------+------------+----+-----+------+\n",
      "|2019-05-27 00:00:00|215         |262         |null|null |3     |\n",
      "|2019-05-29 00:00:00|140         |174         |null|null |1     |\n",
      "|2019-05-29 00:00:00|119         |254         |null|1    |null  |\n",
      "|2019-06-02 00:00:00|220         |241         |null|1    |null  |\n",
      "|2019-05-30 00:00:00|244         |24          |null|7    |3     |\n",
      "|2019-06-04 00:00:00|250         |250         |null|1    |null  |\n",
      "|2019-06-05 00:00:00|246         |112         |null|null |6     |\n",
      "|2019-06-07 00:00:00|145         |137         |null|null |2     |\n",
      "|2019-06-09 00:00:00|236         |100         |null|null |29    |\n",
      "|2019-06-14 00:00:00|98          |264         |null|null |2     |\n",
      "|2019-06-16 00:00:00|66          |138         |null|1    |1     |\n",
      "|2019-06-10 00:00:00|229         |185         |null|null |1     |\n",
      "|2018-01-22 00:00:00|159         |134         |null|1    |null  |\n",
      "|2018-01-22 00:00:00|197         |138         |null|1    |null  |\n",
      "|2018-02-21 00:00:00|159         |229         |null|1    |null  |\n",
      "|2018-02-22 00:00:00|226         |138         |null|null |3     |\n",
      "|2018-03-30 00:00:00|263         |155         |null|null |1     |\n",
      "|2018-04-01 00:00:00|113         |144         |null|null |112   |\n",
      "|2018-04-01 00:00:00|24          |113         |null|null |3     |\n",
      "|2018-05-01 00:00:00|33          |88          |null|5    |1     |\n",
      "|2018-05-15 00:00:00|48          |35          |null|null |3     |\n",
      "|2018-06-19 00:00:00|92          |134         |null|4    |1     |\n",
      "|2018-07-29 00:00:00|141         |170         |null|null |109   |\n",
      "|2018-07-29 00:00:00|69          |74          |null|1    |4     |\n",
      "|2018-07-30 00:00:00|125         |33          |null|null |8     |\n",
      "|2018-07-30 00:00:00|165         |1           |564 |null |null  |\n",
      "|2018-08-02 00:00:00|158         |181         |null|null |9     |\n",
      "|2018-01-30 00:00:00|140         |66          |null|null |4     |\n",
      "|2018-01-30 00:00:00|97          |143         |null|null |1     |\n",
      "|2018-02-24 00:00:00|179         |162         |null|1    |2     |\n",
      "+-------------------+------------+------------+----+-----+------+\n",
      "only showing top 30 rows"
     ]
    }
   ],
   "source": [
    "time_series = count_per_day_resamples.groupBy([\"ts_resampled\", \"pulocationid\", \"dolocationid\"])\\\n",
    ".pivot('type').sum(\"count\").drop(\"null\").cache()\n",
    "time_series.show(30,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8755220"
     ]
    }
   ],
   "source": [
    "time_series.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now that we have 8 million entries -- let's send this back to the local python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we are in the local panda/python environment now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_resampled    datetime64[ns]\n",
       "pulocationid             int64\n",
       "dolocationid           float64\n",
       "fhv                    float64\n",
       "green                  float64\n",
       "yellow                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "time_series.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
